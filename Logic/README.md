# ABench-Logic
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)


## Overview
Stateâ€‘ofâ€‘theâ€‘art LLMs still struggle with formal logic reasoning and fallacy detection. This benchmark addresses a critical gap often overlooked by traditional fact-Â or semantics-based evaluation suites.Â Based on the principle of validity,Â we release a 500â€‘item test suite of discrete logical problems.Â Each task requires a valid inference:Â premisesÂ â†’Â inferenceÂ â†’Â conclusion.Â All problems come from highâ€‘discrimination realâ€‘world logic exams and pass multiple layers of manual curation.

## Key features

ğŸ§ ï¸ **â†’ Expert-crunched**: Every single problem is produced and doubleâ€‘checked by domain experts strictly adheres to rigorous quality standards.
 
ğŸ”’ **â†’ Contamination-proof**: To prevent data contamination, each problem is sourced from thousands of questions and undergoes a rigorous multi-stage verification pipeline by both models and experts.

ğŸ”—  **â†’ RLVR-oriented**: Items retain the structural complexity of premier logic reasoning exams, stressing deep logical inference instead of superficial pattern matching.

ğŸ¯ï¸ **â†’ Accurate evaluation**: Based on the foundational principles of logic, an inference is considered valid if and only if its conclusion is necessarily true whenever its premises are true. Our evaluation directly measure the absolute correctness of the answer with a strict all-or-nothing scoring scheme.


## Liscense

We are releasing this project under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0). This allows for both personal and commercial use, provided that the original license and copyright notice are included in any distributed copies or substantial portions of the software.

We are releasing this project under the Creative Commons Attribution 4.0 International License (CC BY 4.0).

## Evaluation Metrics
We employ a strict, all-or-nothing scoring mechanism, where no partial credit is awarded. A model's response is judged as correct only if it perfectly matches the ground-truth answer. The specific criteria for correctness vary by question type:
* For Multiple-Choice Questions (MCQ):  A response is considered correct if and only if the set of selected options is exactly identical to the set of ground-truth correct options.
* For Question Answering (QA):  The answers to these questions consist of words. A response is judged as correct only if it is an exact match to the standard answer. For questions where the sequence of words is semantically crucial (explicitly marked by an is-order flag in our dataset), this exact match criterion is also order-sensitive.

## Main Results
![Main_Result](img/logic.png)
| Models                       |   Accuracy |
|:-----------------------------|-----------:|
| Gemini2.5-pro-reasoning      |     0.506  |
| Gemini2.5-pro                |     0.502  |
| o3                           |     0.454  |
| DeepSeek-R1                  |     0.412  |
| o1                           |     0.386  |
| QwQ-32B                      |     0.356  |
| Claude-3.7-Sonnet-Thinking   |     0.236  |
| Qwen-Max                     |     0.200  |
| DeepSeek-V3                  |     0.190  |
| GPT-4o                       |     0.190  |
| Claude-3.7-Sonnet            |     0.184  |
| O3mini                       |     0.184  |
| GPT4.1                       |     0.142  |
| DeepSeek-R1-Distill-Qwen-32B |     0.114  |
| Qwen3-30B-A3B                |     0.008  |


* Current SOTA models still struggle with our logical challenge benchmark, failing to reliably solve these problems.


## Data Structure
* The dataset contains 500 logical reasoning problems provided in a structured plain text file.The dataset has two distinct question formats: QA problems and MCQ. The QA entries are designed for tasks where the model must generate a specific word or ordered sequence of words as the answer.The MCQ entries are designed for tasks where the model must select the correct option(s) from a given list. For this question type, there may be one or more correct answers.This section serves as a fixed benchmark for consistent evaluation.
  
    **scheme**
    | mid | standard_question | standard_answer   | type | is_order  | 
    |----|----------|---------------------|------------|-------------|
    | 321 | question_text | answer_text | QA | YES |



## Usage Guide
1. Create an Environment Variables File (if you are using API models)
   ```
    API_KEY=<Your API Key>
    API_URL=<API Endpoint (if you are using a third-party API)
   ```
2. Install requirements
   ```
    pip install -r requirements.txt
   ```
3. Perform Evaluations Only

     If you have already generated LLM results and want to perform evaluations without re-running the model. First, please place the model's answers into a new column, following the format of Result_Logic.csv. Then, simply execute the following command:

    ```
   python src/eval.py \
        --llm_response "R1_response" \
        --result_file  ./samples/Result_Logic.csv
    ```

   --llm_response: specifies the name of the column in the CSV file where the model responses are stored (e.g., "R1_response").

   --result_file: the folder path, Result_Logic.csv, where the results produced by the model are stored. This script will utilize these results for accuracy assessment.


## Example problems
#### Multiple-Choice Questions
```
Question:
åœ¨ä¸€ä¸ªå¤è€çš„éŸ³ä¹åˆå¥æ¯”èµ›ä¸­ï¼Œæœ‰ä¸‰ä½é’¢ç´æ¼”å¥è€…ï¼šå°æ¢…ã€å°ç«¹ã€å°å…°ï¼Œä»¥åŠä¸‰ä½å°æç´æ¼”å¥è€…ï¼šå°è·ã€å°èŠã€å°è²ã€‚ä»–ä»¬å¯ä»¥é€‰æ‹©æ¼”å¥å››ç§éŸ³ä¹ä½œå“ï¼šåå¥æ›²ã€å¥é¸£æ›²ã€äº¤å“ä¹å’Œç‹‚æƒ³æ›²ã€‚å°æ¢…æ¼”å¥çš„æ˜¯åå¥æ›²æˆ–å¥é¸£æ›²ï¼›å°èŠæ¼”å¥ç‹‚æƒ³æ›²ï¼›å¦‚æœä¸€éƒ¨ä½œå“æ²¡æœ‰ä»»ä½•é’¢ç´æ¼”å¥è€…æ¼”å¥ï¼Œé‚£ä¹ˆä»»ä½•å°æç´æ¼”å¥è€…ä¹Ÿä¸èƒ½æ¼”å¥è¯¥ä½œå“ï¼›ä¸€éƒ¨ä½œå“åªæœ‰æœ‰å°æç´æ¼”å¥è€…æ¼”å¥ï¼Œé’¢ç´æ¼”å¥è€…æ‰èƒ½æ¼”å¥è¯¥ä½œå“ï¼›æ¯ä¸ªæ¼”å¥è€…åªèƒ½æ¼”å¥ä¸€éƒ¨ä½œå“ã€‚å¦‚æœé¢˜å¹²çš„æ–­å®šä¸ºçœŸï¼Œä¸”æœ‰äººæ¼”å¥å¥é¸£æ›²ï¼Œåˆ™æ¼”å¥å¥é¸£æ›²çš„æ¼”å¥è€…ä¸­ä¸å¯èƒ½åŒæ—¶åŒ…å«ï¼Ÿ
Aã€å°æ¢…å’Œå°ç«¹
Bã€å°ç«¹å’Œå°å…°
Cã€å°å…°å’Œå°è·
Dã€å°å…°å’Œå°è²
Eã€å°è·å’Œå°è²
Fã€å°æ¢…å’Œå°è·
Gã€å°ç«¹å’Œå°è·
Hã€å°æ¢…å’Œå°è²
Answer: B
is-orderï¼šNO

Question:
On a large ranch called Prairie View Ranch, there are six cowboys named Jake, Tom, Ben, Sam, Cody, and Max. Each cowboy is proficient in two tasks: a primary task and a secondary task. Sam's secondary task is cattle roping. Three of the other cowboys have cattle roping as their primary task. Cody and Max both have horseback riding as one of their tasks. Max's primary task is fence mending, which is a secondary task for both Ben and Cody. Cattle roping and saddle making are Jake's tasks, but the primary-secondary relationship is the reverse of Sam's. Boot repairing is a secondary task for only one of them. The only cowboy with a sister has saddle making as his primary task.
Which one of the following can be true?
Aã€Ben's primary task is cattle roping
Bã€Cody has a sister
Cã€Jake's primary task is saddle making
Dã€Max has a sister
Eã€Sam has a sister
Fã€Max's secondary task is fence mending
Gã€Sam's secondary taks is saddle making
Answer: A E
is-orderï¼šNO

```


#### Question Answering
```
Question:
In the enchanted forest of Eldoria, three elvesâ€”Elara, Thalion, and Lyraâ€”each hold a sacred role: Guardian (always speaks truth), Deceiver (always tells lies), or Mystic (alternates between truth and lies, starting with either). They share the following statements with a traveler:
Elara:Thalion is a Guardian.I am a Mystic.
Thalion:Elara is a Deceiver.I am a Guardian.
Lyra: I am a Guardian. Thalion is a Mystic.
What are the roles of Elara, Thalion, and Lyra respectively?
Answer:  Deceiver Guardian Mystic
is-orderï¼šYES

Question: ç­çº§é‡Œæœ‰å››ä¸ªå­¦ç”Ÿï¼šå°åã€å°ä¸½ã€å°å¼ºã€å°ç¾ã€‚å·²çŸ¥ï¼šå°åè¯´ï¼šâ€œå°ç¾æ˜¯å¹´é¾„æœ€å°çš„ã€‚â€å°ä¸½è¯´ï¼šâ€œæˆ‘æ˜¯å°åçš„å§å§ã€‚â€å°å¼ºè¯´ï¼šâ€œå››ä¸ªäººä¸­åªæœ‰å“¥å“¥æ˜¯ç”·ç”Ÿï¼Œå…¶ä½™éƒ½æ˜¯å¥³ç”Ÿã€‚â€å¦‚æœä¸Šè¿°éƒ½ä¸ºçœŸï¼Œå››ä¸ªå­¦ç”ŸæŒ‰ç…§å¹´é¾„ç”±å¤§åˆ°å°æ’åºæ˜¯ï¼š
Answer: å°ä¸½ å°å å°å¼º å°ç¾
is-orderï¼šYES

Question: In a spelling bee, three contestants - William, James, and Benjamin - are the top three scorers in some order. Each contestant makes two statements about their positions, with one statement being valid and the other being invalid. Here are their statements:William said: "I am not in first place." "James came second."James said: "I am the top scorer." "Benjamin was second."Benjamin said: "I am not the winner." "William finished third."
Which one is in second place?
Answer: William
is-orderï¼šNO

```



