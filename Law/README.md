# ABench-Psychology
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)


## Overview
LawBench is a benchmark comprising 500 challenging legal questions designed to evaluate models' capabilities in law. The test includes Multiple-Choice Questions (MCQ) and Question Answering (QA). The 500 questions cover Civil Law, Civil Procedure Law, Criminal Law, Criminal Procedure Law, Administrative Law, and International Law. Knowledge points are derived from recent judicial examinations, and new questions are developed based on these points and thoroughly reviewed by experts for accuracy.

## Key features
ğŸ§  **â†’ Test data leakage prevention:**:  Curated from over 10,000 authoritative questions and refined through model pre-screening, human cleaning, model rewrites, and expert verification, the dataset is designed to minimize any trainâ€“test leakage.

ğŸ›¡ï¸ **â†’ Expert-grade quality control**: Every question is created and repeatedly reviewed by experts under strict quality standards.

ğŸ¤  **â†’ RLVR-oriented**: Core knowledge points of judicial exams are retained while increasing question complexity to emphasize both depth and breadth.

ğŸ“‹ï¸ **â†’ Precise evaluation**: Each short-answer question has a unique answer, which is simplified as much as possible during design so that a matching response is considered correct.


## Liscense

We are releasing this project under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0). This allows for both personal and commercial use, provided that the original license and copyright notice are included in any distributed copies or substantial portions of the software.

We are releasing this project under the Creative Commons Attribution 4.0 International License (CC BY 4.0).

## Evaluation Metrics
We strictly use an "all or nothing" scoring mechanism, which means the model's answer is judged correct only when it completely matches the standard answer or scoring criteria. The specific accuracy standards vary depending on the question typeï¼š
* For Multiple-Choice Questions (MCQ): The answer is considered correct only when the option set completely matches the standard answer set.
* For Question Answering (QA): These questions require open-ended answers. We employ a model-as-judge to score the responses, strictly adhering to each item outlined in the LLM_judge_promt. 

## Main Results
![Main_Result](img/law.png)
| Models                                |   Accuracy |
|:--------------------------------------|-----------:|
| Gemini2.5-pro                         |     0.400  |
| Gemini2.5-pro-reasoning               |     0.398  |
| QwQ-32B                               |     0.370  |
| DeepSeek-R1                           |     0.366  |
| o3                                    |     0.328  |
| Claude-3.7-Sonnet-20250219-Thinking   |     0.304  |
| o1                                    |     0.250  |
| Qwen3-30B-A3B                         |     0.244  |
| Qwen-Max                              |     0.208  |
| DeepSeek-V3                           |     0.198  |
| DeepSeek-R1-Distill-Qwen-32B          |     0.184  |
| Claude-3.7-Sonnet                     |     0.170  |
| GPT4.1                                |     0.132  |
| GPT-4o                                |     0.122  |
| O3mini                                |     0.110  |




* Current SOTA models still struggle with our psychology challenge benchmark, failing to reliably solve these problems.


## Data Structure
The dataset contains 500 legal questions, provided as structured plain-text files: 180 QA and 320 multiple-choice (MCQ).The QA entries are designed for tasks where the model generates open-ended answers, with scoring based on the standard answers.The MCQ entries are designed for tasks where the model must select the correct option from a given list. For this question type, there is one and only one correct answer.

**scheme**
   | mid | standard_question | standard_answer   | type | is_order  | 
   |----|----------|---------------------|------------|-------------|
   | 321 | question_text | answer_text | QA | YES |


## Usage Guide
1. Create an Environment Variables File (if you are using API models)
   ```
    API_KEY=<Your API Key>
    API_URL=<API Endpoint (if you are using a third-party API)>
   ```
2. Install requirements
   ```
    pip install -r requirements.txt
   ```
3. Perform Evaluations Only

     If you have already generated LLM results and want to perform evaluations without re-running the model. First, please place the model's answers into a new column, following the format of Result_Law.csv. Then, simply execute the following command:

    ```
   python src/eval.py \
        --llm_response "R1_response" \
        --result_file  ./samples/Result_Law.csv \
        --output_file ../evaluation_details.csv
    ```


   --llm_response: specifies the name of the column in the CSV file where the model responses are stored (e.g., "R1_response").

   --result_file: the folder path, where the results produced by the model are stored. This script will utilize these results for accuracy assessment.
   

## Example problems
#### Multiple-Choice Questions
```
Question: æŸå¸‚åŸå»ºé›†å›¢é¡¹ç›®ç»ç†ç”²ï¼ˆç”·ï¼Œ45å²ï¼Œæœ¬ç§‘å­¦å†ï¼Œç±è´¯æ±Ÿå—çœç™½æ²³å¸‚ï¼Œæ€§æ ¼è°¨æ…ï¼Œä¸šä½™çˆ±å¥½é’“é±¼ï¼‰ï¼Œåˆ©ç”¨èŒåŠ¡ä¾¿åˆ©åœ¨2021å¹´3æœˆè‡³2023å¹´6æœˆæœŸé—´ï¼Œé€šè¿‡ç­¾è®¢è™šå‡å·¥ç¨‹å’¨è¯¢åˆåŒï¼ˆåˆåŒåç§°ä¸ºã€Šç»¿åŸé¡¹ç›®æŠ€æœ¯æŒ‡å¯¼åè®®ã€‹ï¼Œå…±8é¡µï¼Œäº2022å¹´9æœˆ15æ—¥åœ¨åŸå»ºé›†å›¢ä¸‰æ¥¼ä¼šè®®å®¤ç­¾ç½²ï¼Œè§è¯äººä¸ºç§˜ä¹¦å°ç‹ï¼Œäº¤ä»˜è¿‡ç¨‹é‡‡ç”¨é¡ºä¸°å¿«é€’å¯„é€ï¼‰çš„æ–¹å¼å¤šæ¬¡æ”¶å—è´¿èµ‚ã€‚2023å¹´7æœˆ12æ—¥æ˜ŸæœŸä¸‰ä¸‹åˆ3æ—¶è®¸ï¼Œé˜´å¤©ä¼´æœ‰é—´æ–­å°é›¨ï¼Œç”²å› ä¸‹å±ä¹™ï¼ˆç”·ï¼Œ28å²ï¼ŒåŠ©ç†å·¥ç¨‹å¸ˆï¼Œä¹ æƒ¯ç©¿ç™½è‰²å®‰è¸è¿åŠ¨é‹å’Œç°è‰²æ£‰è´¨ä¼‘é—²è£¤ï¼‰å‘çºªæ£€éƒ¨é—¨ä¸¾æŠ¥å…¶è¿æ³•è¡Œä¸ºï¼Œåœ¨åŸå»ºé›†å›¢åŠå…¬æ¥¼506åŠå…¬å®¤ï¼ˆå†…éƒ¨é“ºæœ‰ç±³è‰²å¤§ç†çŸ³åœ°ç –ï¼Œå¢™é¢æ‚¬æŒ‚å·¥ç¨‹è§„åˆ’å›¾ï¼Œé…æœ‰çº¢æœ¨åŠå…¬æ¡Œå’Œé»‘è‰²çš®è´¨è½¬æ¤…ï¼‰å†…æ¥å›è¸±æ­¥ä¸‰åœˆåï¼Œç”¨åä¸ºMate40æ‰‹æœºè”ç³»ç¤¾ä¼šäººå‘˜å¼ æŸï¼Œå‹ä½å£°éŸ³è¯´é“ï¼šâ€å¤©é»‘å‰æŠŠäº‹åŠå¦¥ï¼Œåˆ«ç”¨ä¸Šæ¬¡é‚£è¾†ç™½è‰²é¢åŒ…è½¦â€œã€‚æ¬¡æ—¥å‡Œæ™¨ï¼Œä¹™åœ¨å›å®¶é€”ç»å…‰æ˜è·¯ä¸æ«æ—è¡—äº¤å‰å£æ—¶ï¼Œè¢«å¼ æŸæŒé’¢ç®¡å‡»æ‰“è…¿éƒ¨è‡´ç²‰ç¢æ€§éª¨æŠ˜ï¼ˆç»é‰´å®šä¸ºé‡ä¼¤äºŒçº§ï¼‰ï¼ŒæœŸé—´ä¹™çš„æ·±è“è‰²å¸†å¸ƒèƒŒåŒ…å†…è£…æœ‰æœªæ‹†å°çš„ä¸‰åªæ¾é¼ åšæœç¤¼ç›’æ‰è½åœ¨åœ°ã€‚å…³äºæœ¬æ¡ˆåº­å‰ä¼šè®®ï¼Œä¸‹åˆ—å“ªäº›é€‰é¡¹ä¸æ­£ç¡®ï¼Ÿ 
A.ç”²å¯å°±æ¡ˆä»¶ç®¡è¾–æå‡ºå¼‚è®® 
B.ä¹™æèµ·é™„å¸¦æ°‘äº‹è¯‰è®¼çš„ï¼Œå¯è°ƒè§£ 
C.ç”²æå‡ºå…¶å£ä¾›ç³»åˆ‘è®¯æ‰€å¾—ï¼Œæ³•å®˜å¯åœ¨å®¡æŸ¥è®¯é—®æ—¶åŒæ­¥å½•åƒçš„åŸºç¡€ä¸Šå†³å®šæ˜¯å¦æ’é™¤å£ä¾› 
D.åº­å‰ä¼šè®®ä¸Šå‡ºç¤ºè¿‡çš„è¯æ®ï¼Œåº­å®¡æ—¶ä¸¾è¯ã€è´¨è¯åº”ç®€åŒ–
Answer: C D

Question: æŸæ³•å­¦æ•™æˆç‹æŸï¼ˆ45å²ï¼Œä¸­å›½äººæ°‘å¤§å­¦æ³•å­¦åšå£«ï¼Œä¸»è¦ç ”ç©¶åˆ‘äº‹è¯‰è®¼æ³•å­¦é¢†åŸŸï¼‰ï¼Œæ¥å—æŸå¹³å°çš„è¦æ±‚ï¼Œä¸ºå­¦å‘˜ä»¬æä¾›åˆ‘äº‹è¯‰è®¼ç›¸å…³çš„ç›´æ’­è¯¾ã€‚ç‹æŸåœ¨ç›´æ’­é—´å¼€å¯äº†è§†é¢‘ï¼Œå°†æ‘„åƒå¤´ã€éŸ³å“ç­‰è¿›è¡Œäº†è°ƒæ•´ï¼Œç”±ç›´æ’­é—´å·¥ä½œäººå‘˜å®£è¯»äº†ç›´æ’­è¯¾çš„çºªå¾‹åï¼Œç›´æ’­æ­£å¼å¼€å§‹ã€‚ç‹æŸåœ¨ç½‘ç»œç›´æ’­è¿‡ç¨‹ä¸­ï¼Œä¸å„å¬è¯¾çš„å­¦å‘˜å±•å¼€äº†ç§¯æçš„äº’åŠ¨ã€‚åœ¨æåˆ°åˆ‘äº‹è¯‰è®¼çš„è¯æ˜ä¸»ä½“æ—¶ï¼Œå„å­¦å‘˜çº·çº·åœ¨ç›´æ’­é—´ä¸­å‘è¡¨äº†å„è‡ªçš„è§‚ç‚¹ã€‚ å…³äºæˆ‘å›½åˆ‘äº‹è¯‰è®¼çš„è¯æ˜ä¸»ä½“ï¼Œä¸‹åˆ—é€‰é¡¹æ˜¯é”™è¯¯çš„æ˜¯ï¼Ÿ A.æ•…æ„æ¯åè´¢ç‰©æ¡ˆä¸­çš„é™„å¸¦æ°‘äº‹è¯‰è®¼åŸå‘Šäººæ˜¯è¯æ˜ä¸»ä½“ 
B.ä¾µå æ¡ˆä¸­æèµ·åè¯‰çš„è¢«å‘Šäººæ˜¯è¯æ˜ä¸»ä½“ 
C.å¦¨å®³å…¬åŠ¡æ¡ˆä¸­å°±æ‰§è¡ŒèŒåŠ¡æ—¶ç›®å‡»çš„çŠ¯ç½ªæƒ…å†µå‡ºåº­ä½œè¯çš„è­¦å¯Ÿæ˜¯è¯æ˜ä¸»ä½“
D.è¯æ˜ä¸»ä½“éƒ½æ˜¯åˆ‘äº‹è¯‰è®¼ä¸»ä½“  
Answer: C

```

#### Question Answering
```
Question: æ²³è¥¿çœç»¿æºå¸‚å†œæœºé”€å”®ä¸ªä½“æˆ·ææœï¼ˆ32å²ï¼Œé«˜ä¸­æ–‡åŒ–ç¨‹åº¦ï¼Œæ€§æ ¼æ€¥èºï¼Œä¸šä½™çˆ±å¥½é’“é±¼å’Œé˜…è¯»å†œä¸šç§‘æŠ€æ‚å¿—ï¼‰ã€‚å…¶çˆ¶å› å¿ƒè„ç—…ä½é™¢æ²»ç–—æ€¥éœ€åŒ»ç–—è´¹ï¼Œææœå†³å®šå‡ºå”®è‡ªç”¨çš„ä¸œæ–¹çº¢ç‰Œçº¢è‰²è”åˆæ”¶å‰²æœºï¼ˆä¸ƒæˆæ–°ï¼Œè´­äº2015å¹´æ˜¥ï¼Œå¹³æ—¶åœæ”¾åœ¨ç»¿æºå¸‚å†œæœºå¸‚åœºä¸œä¾§å‡ºç§Ÿæˆ¿å†…ï¼Œè¯¥æˆ¿ä¸ºå·¥ä¸šé£æ ¼è£…ä¿®ï¼Œå¢™é¢è£¸éœ²çº¢ç –ï¼Œåœ°é¢å †æ”¾ç€äºŒæ‰‹å†œæœºé›¶ä»¶å’Œé“è´¨è´§æ¶ï¼‰ã€‚2017å¹´3æœˆ1æ—¥é˜´æœ‰å°é›¨çš„ä¸Šåˆ9æ—¶è®¸ï¼Œææœåœ¨å…¶å‡ºç§Ÿæˆ¿å†…ç”¨é»‘è‰²è‹±é›„ç‰Œç­¾å­—ç¬”èƒè¿«å‰æ¥æ´½è°ˆçš„å¼ æˆï¼ˆ28å²ï¼Œç»¿æºå¸‚å†œèµ„å…¬å¸é€è´§å‘˜ï¼‰ç­¾è®¢åˆåŒï¼Œçº¦å®šè´§åˆ°ä»˜æ¬¾ï¼ŒåˆåŒç­¾è®¢æ—¶çª—å¤–ä¼ æ¥éš”å£äº”é‡‘åº—çš„ç”µé’»å£°ï¼›4æœˆ1æ—¥å¤šäº‘è½¬æ™´çš„ä¸‹åˆ3ç‚¹ï¼Œææœä¸ç¨‹åŠ›åœ¨è¯¥å‡ºç§Ÿæˆ¿äºŒæ‰‹æœ¨æ¡Œå‰ç­¾è®¢ã€Šæ”¶å‰²æœºä¹°å–åˆåŒã€‹ï¼ˆå…±8é¡µï¼Œè§è¯äººä¸ºå¸‚åœºä¿å®‰èµµæ–Œï¼ŒåˆåŒæ‰«æä»¶é€šè¿‡å¾®ä¿¡å‘é€ï¼‰ï¼Œç¨‹åŠ›å½“åœºæ”¯ä»˜20%è´§æ¬¾ï¼›5æœˆ1æ—¥æ™´æœ—çš„å‘¨æœ«ä¸Šåˆï¼Œé«˜è¿›ç©¿ç€æ·±è“è‰²æ¶¤çº¶å·¥è£…è£¤å‰å¾€ææœä½å¤„ï¼Œåœ¨å †æ»¡å†œæœºç»´ä¿®æ‰‹å†Œçš„èŒ¶å‡ ä¸Šæ”¯ä»˜å…¨æ¬¾ï¼›6æœˆ1æ—¥é—·çƒ­çš„å‚æ™šï¼Œææœå°†æ“¦æ‹­ä¸€æ–°çš„æ”¶å‰²æœºäº¤ä»˜ç»™é¡¾å®¶ï¼Œäº¤ä»˜æ—¶æ”¶å‰²æœºé’¥åŒ™ä¸ŠæŒ‚ç€åˆ»æœ‰â€œå¹³å®‰â€å­—æ ·çš„é“œè´¨é’¥åŒ™æ‰£ã€‚ä¸Šè¿°ä¹°å—äººå‡è¦æ±‚å®é™…å±¥è¡ŒåˆåŒï¼Œå°±å±¥è¡Œé¡ºåºäº§ç”Ÿäº‰è®®ã€‚å¼ æˆã€ç¨‹åŠ›ã€é«˜è¿›ã€é¡¾å®¶å››äººå±¥è¡Œé¡ºåºæ˜¯ä»€ä¹ˆï¼Ÿ
Answer:é¡¾å®¶ã€ç¨‹åŠ›ã€é«˜è¿›ã€å¼ æˆ 


```



